{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment-BERT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzTINeLp1aBD",
        "colab_type": "text"
      },
      "source": [
        "# **CamemBERT**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CevacBEycWG",
        "colab_type": "text"
      },
      "source": [
        "[CamemBERT](https://camembert-model.fr/) est un modèle de traitement automatique de la langue Française, basé sur l'architecture [RoBERTa](https://ai.facebook.com/blog/roberta-an-optimized-method-for-pretraining-self-supervised-nlp-systems/) développée par Facebook AI en 2019, dédiée à l'Anglais. RoBERTa est lui même basé sur [BERT](https://fr.wikipedia.org/wiki/BERT_(mod%C3%A8le_de_langage)) qui a été développé par Google en 2018.  \n",
        "CamemBERT est donc un cousin Français de BERT, qui a pu voir le jour lorsque les équipes de Facebook associés aux chercheurs de [l'INRIA](https://www.inria.fr/fr) ont rendu public ce modèle pré-entrainé sur 138GB de texte Français.  \n",
        "CamemBERT a été pré-entraîné sur un corpus francophone et avec des hyper-paramètres différents découverts et testés pour la première fois par l’équipe de Facebook. Le choix de ces hyper-paramètres était tellement réussi que l’entreprise a annoncé le sortie d’un “nouveau” modèle baptisé RoBERTa. Pourtant, il n’y a rien de nouveau dans RoBERTA qui comme CamemBERT reste une copie de BERT. Voici ces hyper-paramètres:  \n",
        "*  CamemBERT choisit les mots à prédire de manière dynamique, c’est-à-dire, non pas lors du pré-processing des données en entrée, mais lors de forward pass, en masquant au hasard certains mots d’une séquence.  \n",
        "*  Il utilise un batch size différent: ~8 000 contre 256 dans le cas de BERT.  \n",
        "*  CamemBERT a un seul objectif de pré-entrainement: prédiction des “mots masqués” d’une séquence. BERT en avait deux : prédiction des “mots masqués” et de la phrase suivante d’une séquence. Ce dernier objectif s’est avéré improductif pour l’entrainement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aeHO-8w11jm",
        "colab_type": "text"
      },
      "source": [
        "# **Exemple d'utlisation de CamemBERT**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdqHiJ0_1-yr",
        "colab_type": "text"
      },
      "source": [
        "CamemBERT a été entrainé dans le but de prédire des \"mots masqués\" dans un texte. Nous allons voir un exemple de ce que peut faire ce modèle pré-entrainé."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDlpYuxU2v6B",
        "colab_type": "text"
      },
      "source": [
        "Commençons par installer sur la machine les modules python dont nous aurons besoin et qui ne sont pas pré-installés. Ensuite nous importons les bibliothèques.  \n",
        "En particulier, nous utiliserons la bibliothèque [Transformers](https://huggingface.co/transformers/#) créée par [Hugging Face](https://huggingface.co/). Cette bibliothèque contient des centaines de modèles pré-entrainés pour réaliser des opérations sur les données textuelles, comme la classification, l'extraction d'informations, le \"questions-réponses\", la traduction, ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdwgQ7MfNoj2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "5e083caf-f86e-4728-b089-26409c31db54"
      },
      "source": [
        "!pip install transformers --quiet"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 890kB 3.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 15.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 13.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.0MB 25.0MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJzvJaBONyHe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from transformers import TFCamembertForMaskedLM\n",
        "from transformers import AutoTokenizer"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mV2cj3KN4rDm",
        "colab_type": "text"
      },
      "source": [
        "Définisson la variable tokenizer qui permettra d'instancier le tokenizer pour CamemBERT fourni par la librairie \"transformers\" :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "772aq9lI41KX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('jplu/tf-camembert-base',)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fI43XfZ4JuL",
        "colab_type": "text"
      },
      "source": [
        "Définissons maintenant la phrase que nopus souhaitons compléter puis utlisons le tokenizer de la bibliothèque \"transformers\" pour préparer le texte :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9mWdtCE9NAP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "9238f23b-5632-4358-82ce-ee2dbb356100"
      },
      "source": [
        "phrase = \"L'intelligence artificielle va mener à la <mask> du monde !\"\n",
        "output_tokenizer = tokenizer.encode_plus(phrase, max_length=100, padding=\"longest\", truncation=True, return_tensors='tf')\n",
        "output_tokenizer"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': <tf.Tensor: shape=(1, 14), dtype=int32, numpy=\n",
              "array([[    5,    71,    11,  6031,  7956,   198,  3532,    15,    13,\n",
              "        32004,    25,   164,    83,     6]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 14), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENYtpJbd5qRd",
        "colab_type": "text"
      },
      "source": [
        "Récupérons la position du masque (mask) dans la séquence retournée par le tokenizer :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWEF1kRt5-EW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2b58254b-8aff-4c0b-9edb-ecf1bc670de1"
      },
      "source": [
        "mask_index = (output_tokenizer['input_ids'][0].numpy() == tokenizer.mask_token_id).nonzero()\n",
        "mask_index = np.reshape(mask_index,(1))[0]\n",
        "mask_index"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3luydxK483g",
        "colab_type": "text"
      },
      "source": [
        "Instancions maintenant le modèle CamemBERT pré-entrainé avec le jeu de données de base pour Tensorflow :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtZniccuxmi2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "a2e21538-e5b5-4b65-dbea-921f2c782d60"
      },
      "source": [
        "model = TFCamembertForMaskedLM.from_pretrained(\"jplu/tf-camembert-base\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint weights were used when initializing TFCamembertForMaskedLM.\n",
            "\n",
            "All the weights of TFCamembertForMaskedLM were initialized from the model checkpoint at jplu/tf-camembert-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFCamembertForMaskedLM for predictions without further training.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8upQ4QXi9Hbw",
        "colab_type": "text"
      },
      "source": [
        "Puis lançons le modèle sur la phrase :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXVhMcTk25xQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "4a852739-c796-4587-b803-100ac58fa696"
      },
      "source": [
        "output = model(output_tokenizer['input_ids'])[0]\n",
        "output"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 14, 32005), dtype=float32, numpy=\n",
              "array([[[ 23.108753  ,  -3.755758  ,   8.674742  , ...,  -5.096422  ,\n",
              "          -4.313377  ,   2.3206162 ],\n",
              "        [ -2.5845957 ,  -3.958022  ,  14.981613  , ...,  -7.429527  ,\n",
              "          -1.1563265 ,  -3.0757904 ],\n",
              "        [  1.1122129 ,  -7.8049626 ,   4.1219034 , ..., -12.544399  ,\n",
              "          -7.452759  ,  -4.76741   ],\n",
              "        ...,\n",
              "        [  4.594441  ,  -8.668797  ,   9.463923  , ..., -14.600243  ,\n",
              "         -14.187639  ,   0.7320218 ],\n",
              "        [  3.9691653 ,  -8.347343  ,  11.592593  , ...,  -9.434902  ,\n",
              "          -9.269459  ,  -0.75447655],\n",
              "        [  9.361578  ,  -4.3270473 ,  27.006542  , ...,  -6.9380293 ,\n",
              "          -6.461198  ,   3.1999168 ]]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AS_IKc1CHb8U",
        "colab_type": "text"
      },
      "source": [
        "On récupère les (32005 !) valeurs en sortie du modèle correspondants à l'emplacement du masque :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HbqsyMHQQT8Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "dde3b924-e405-4d6c-cebb-052a3c83880b"
      },
      "source": [
        "output  = output[0, mask_index, :]\n",
        "output"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(32005,), dtype=float32, numpy=\n",
              "array([ 1.0191815, -2.3711014,  1.6898267, ..., -8.149946 , -5.941703 ,\n",
              "       -2.0453548], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7Gdzo6TH4-A",
        "colab_type": "text"
      },
      "source": [
        "Puis on applique une fonctioon d'activation Soft-Max afin de normaliser les probabilités sur chaque valeurs :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpgh6OwtQ2FO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "72542be4-25dd-4e48-fe3b-c1791638a01a"
      },
      "source": [
        "proba = tf.nn.softmax(output)\n",
        "proba"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(32005,), dtype=float32, numpy=\n",
              "array([2.1088415e-08, 7.1066097e-10, 4.1238351e-08, ..., 2.1975666e-12,\n",
              "       1.9997254e-11, 9.8431119e-10], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuOhpSQFIVIY",
        "colab_type": "text"
      },
      "source": [
        "On récupère ensuite les valeurs et les indices des 5 plus grandes probabilités parmi ces 32005 probabilités :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8phoc1vLR9ej",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "18fafcb5-39de-4db7-b235-a48ebb9fd5b2"
      },
      "source": [
        "top_proba, top_indices = tf.math.top_k(proba,k=5)\n",
        "print(top_proba)\n",
        "print(top_indices)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([0.6222108  0.08322746 0.05718182 0.03852326 0.02997597], shape=(5,), dtype=float32)\n",
            "tf.Tensor([ 259 9408 1691 8710 5876], shape=(5,), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsz7jaXjIjhr",
        "colab_type": "text"
      },
      "source": [
        "On va chercher ensuite les valeurs numériques des mots correspondants aux emplacements de ces probibilités les plus fortes, puis on reconvertit ces valeurs en mots réels (fonction inverse du tokenizer) :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5z_jPAvTktU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "534da9f2-779e-4d19-f16f-897d8608f4a4"
      },
      "source": [
        "topk_predicted_token_bpe2 = \" \".join([tokenizer.convert_ids_to_tokens(int(tf.keras.backend.get_value(top_indices[i]))) for i in range(len(top_indices))])\n",
        "topk_predicted_token_bpe2 "
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'▁fin ▁conquête ▁découverte ▁domination ▁destruction'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWJm87fFI9UB",
        "colab_type": "text"
      },
      "source": [
        "On place dans la variable \"mask\" le mot clé utlisé dans la phrase pour le masque :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50TaMa6noF6r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3e1240f0-edb7-409e-8b5d-ab8b087d3ada"
      },
      "source": [
        "mask = tokenizer.mask_token\n",
        "mask"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<mask>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uc_1FBxkJJsT",
        "colab_type": "text"
      },
      "source": [
        "On sépare les résultats obtenus :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NScwVqbvqvK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2ec23de5-1848-445a-9d1d-e72359c97b78"
      },
      "source": [
        "topk_predicted_token_bpe2.split(\" \")"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['▁fin', '▁conquête', '▁découverte', '▁domination', '▁destruction']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9gHN2YJJPoI",
        "colab_type": "text"
      },
      "source": [
        "Puis on remplace le masque dans la phrase initiale, en utlisant toutes les possibilités trouvées :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jD_3OKp8vfkz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "3b996bb8-7865-4ec6-b7f1-4aaf9e162e04"
      },
      "source": [
        "topk_filled_outputs_ = []\n",
        "for index2, predicted_token_bpe2 in enumerate(topk_predicted_token_bpe2.split(\" \")):\n",
        "  predicted_token_ = predicted_token_bpe2.replace(\"\\u2581\", \" \")\n",
        "  if \" {0}\".format(tokenizer.mask_token) in phrase:\n",
        "    topk_filled_outputs_.append((phrase.replace(\" {0}\".format(tokenizer.mask_token), predicted_token_)))\n",
        "  else:\n",
        "    topk_filled_outputs_.append((phrase.replace(tokenizer.mask_token, predicted_token_)))\n",
        "\n",
        "topk_filled_outputs_"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"L'intelligence artificielle va mener à la fin du monde !\",\n",
              " \"L'intelligence artificielle va mener à la conquête du monde !\",\n",
              " \"L'intelligence artificielle va mener à la découverte du monde !\",\n",
              " \"L'intelligence artificielle va mener à la domination du monde !\",\n",
              " \"L'intelligence artificielle va mener à la destruction du monde !\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    }
  ]
}