{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNz9XE6cBgT4e+DQ9hni36u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexandreBourrieau/ML-F1/blob/master/Carnets%20Jupyter/Ressentis_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJih03NWtfi0"
      },
      "source": [
        "# **Classification de ressentis avec distilBERT**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6Xo1x4rtto8"
      },
      "source": [
        "L'objectif est de créer un modèle qui prend en entrée des commentaires (en Anglais) et attribue à chacun un ressenti positif ou négatif.  \n",
        "Le modèle est composé de deux parties :  \n",
        "* DistilBERT va encoder le commentaire et en extraire des informations qui seront passées ensuite au réseau de neurones.  \n",
        "* Le modèle suivant est un réseau de neurones qui sera créé avec Keras.  \n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/AlexandreBourrieau/ML-F1/master/Carnets%20Jupyter/Images/StructureBERT.png\" />  \n",
        "  \n",
        "  Les données qui s'échangent entre les deux modèles sont des vecteurs de dimension 768. Ces vecteurs sont l'équivalent de l'application d'un algorithme de prolongation lexicale sur les mots qui composent le commentaire."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gePh7FotwPTH"
      },
      "source": [
        "# **Installation et importations des librairies**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5WrrT_pzIHu",
        "outputId": "0fdb6cac-789a-4159-f3d8-c7b7ad03e57f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "!pip install transformers --quiet"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.1MB 4.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.0MB 12.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 51.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 35.8MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5n4uHtutPlJ"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Dropout, Input, Dropout, Lambda\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from transformers import DistilBertConfig\n",
        "from transformers import TFDistilBertModel\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import random"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWZEaivhxTd2"
      },
      "source": [
        "# **Importation des données**\n",
        "\n",
        "On utilise la librairie pandas pour lire les données depuis le fichier csv disponible sur le site de [standford](https://nlp.stanford.edu/sentiment/index.html) qui contient des commentaires sur des films, chacun d'eux avec une note positive (1) ou négative (0)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOrjCZGyyH4P"
      },
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/AlexandreBourrieau/ML-F1/master/Carnets%20Jupyter/Donn%C3%A9es/train.csv', delimiter='\\t', header=None)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiO-lNQ6zYpU"
      },
      "source": [
        "Affiche quelques informations :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQT1SwyBzPvP",
        "outputId": "3b4d908d-74e5-48a2-aba3-1262814260e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        }
      },
      "source": [
        "print(df[0:10])\n",
        "print(\"Total des données : \", str(len(df)))\n",
        "print(\"Nombre d'avis positifs et négatifs : \",df[1].value_counts())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                   0  1\n",
            "0  a stirring , funny and finally transporting re...  1\n",
            "1  apparently reassembled from the cutting room f...  0\n",
            "2  they presume their audience wo n't sit still f...  0\n",
            "3  this is a visually stunning rumination on love...  1\n",
            "4  jonathan parker 's bartleby should have been t...  1\n",
            "5  campanella gets the tone just right funny in t...  1\n",
            "6  a fan film that for the uninitiated plays bett...  0\n",
            "7  b art and berling are both superb , while hupp...  1\n",
            "8  a little less extreme than in the past , with ...  0\n",
            "9                       the film is strictly routine  0\n",
            "Total des données :  6920\n",
            "Nombre d'avis positifs et négatifs :  1    3610\n",
            "0    3310\n",
            "Name: 1, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4Vcm5K3Dej4"
      },
      "source": [
        "# **Préparation des données**\n",
        "\n",
        "# Séparation des données d'entrainement et de test \n",
        "On commence par récupérer 75% des données pour l'entrainement, et le reste pour les tests :\n",
        "\n",
        "<img src=\"https://github.com/AlexandreBourrieau/ML-F1/blob/master/Carnets%20Jupyter/Images/SeparationData.png?raw=true\"/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XefRRLg2DiQs"
      },
      "source": [
        "# Chargement des commentaires et des ressentis\n",
        "commentaires = df[0].astype(str).tolist()    # Récupère tous les commentaires dans une liste python\n",
        "ressentis = df[1].tolist()                   # Récupère tous les ressentis dans une liste python\n",
        "labels = np.asarray(ressentis)               # Créé un tableau de type numpy avec les ressentis\n",
        "\n",
        "x_entrainement, x_test, y_entrainement, y_test = train_test_split(commentaires, labels, test_size=0.25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDDdUWj83wuz"
      },
      "source": [
        "# Tokénisation  \n",
        "La première étape est de tokéniser les commentaires : les mots sont décomposés en index numériques au format BERT.  \n",
        "\n",
        "<img src=\"https://github.com/AlexandreBourrieau/ML-F1/blob/master/Carnets%20Jupyter/Images/TokenizeBERT.png?raw=true\"/>\n",
        "\n",
        "Après tokénisation, on obtient une liste de séquences et chaque séquence représente une liste d'index. On souhaite que BERT analyse toutes les séquences en une seule fois (ce qui est plus rapide). Il faut donc que toutes les séquences aient la même taille. On va donc ajouter du bourrage pour égaliser la longueur des séquences. Cela est indiqué avec le paramètre `padding='True'`.  \n",
        "Lorsque un bourrage est ajouté, il faut que BERT ne prenne pas en compte les mots à cette position (car il n'y en a pas !). Cette restriction est réalisé grace à l'`attention_mask`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHhqUPeiqzam"
      },
      "source": [
        "MAX_SEQUENCE_LENGTH = 100\n",
        "\n",
        "# Instanciation du tokeniseur\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Préparation des données d'entrainement\n",
        "output_tokenizer_entrainement = tokenizer(x_entrainement,max_length=MAX_SEQUENCE_LENGTH, padding='max_length', truncation=True, return_tensors='tf',add_special_tokens=True)\n",
        "\n",
        "# Préparation des données de tests\n",
        "output_tokenizer_tests = tokenizer(x_test,max_length=MAX_SEQUENCE_LENGTH, padding='max_length', truncation=True, return_tensors='tf',add_special_tokens=True)"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzboG3fX5LkP"
      },
      "source": [
        "Regardons un peu comment sont formatées les données en sortie du tokéniseur :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0BL0iiKzVS7",
        "outputId": "8326fbf5-69ce-423e-8550-f49c38329a65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        }
      },
      "source": [
        "output_tokenizer_entrainement"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': <tf.Tensor: shape=(5190, 100), dtype=int32, numpy=\n",
              "array([[ 101, 2062, 1997, ...,    0,    0,    0],\n",
              "       [ 101, 4299, 2100, ...,    0,    0,    0],\n",
              "       [ 101, 2000, 2488, ...,    0,    0,    0],\n",
              "       ...,\n",
              "       [ 101, 1996, 6530, ...,    0,    0,    0],\n",
              "       [ 101, 3998, 2086, ...,    0,    0,    0],\n",
              "       [ 101, 2002, 3084, ...,    0,    0,    0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(5190, 100), dtype=int32, numpy=\n",
              "array([[1, 1, 1, ..., 0, 0, 0],\n",
              "       [1, 1, 1, ..., 0, 0, 0],\n",
              "       [1, 1, 1, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [1, 1, 1, ..., 0, 0, 0],\n",
              "       [1, 1, 1, ..., 0, 0, 0],\n",
              "       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fejN7ZrzzaOu"
      },
      "source": [
        "Regardons comment le premier commentaire a été encodé :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-E6oJcG5Qlj"
      },
      "source": [
        "print(\"Commentaire original :\", x_entrainement[0])\n",
        "print(\"input_ids: \", output_tokenizer_entrainement['input_ids'][0])\n",
        "print(\"attention_mask: \", output_tokenizer_entrainement['attention_mask'][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAmGZYP86Hx7"
      },
      "source": [
        "Regardons les 5 premiers résultats de la tokénisation : On peut identifier les mot-clés **[CLS]** (valeur : 101) et **[SEP]** (valeur : 102)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWVaSBqo59qW",
        "outputId": "9cf06f11-c37a-484f-c414-787667430d88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "for i in range (0,5):\n",
        "  print(output_tokenizer_entrainement['input_ids'][i])"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[  101  2062  1997  1996  2168  2214 13044  5365  2038  2042  2667  2000\n",
            "  3413  2125  2004 11701  9458  4024  2005  2070  2051  2085   102     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0], shape=(100,), dtype=int32)\n",
            "tf.Tensor(\n",
            "[ 101 4299 2100 9378 2100  102    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0], shape=(100,), dtype=int32)\n",
            "tf.Tensor(\n",
            "[  101  2000  2488  3305  2339  2023  2106  1050  1005  1056  7532  2007\n",
            "  2033  2052  5478  2178 10523  1010  1998  1045 24185  1050  1005  1056\n",
            "  2022  3564  2083  2023  2028  2153  2008  1999  2993  2003  8570  2438\n",
            "   102     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0], shape=(100,), dtype=int32)\n",
            "tf.Tensor(\n",
            "[  101  2009  3957  3532 11271  2482 12417  2498  2000  2079  2008  2003\n",
            "  2428  6057  1010  1998  2059 24273  2149  2000  4756  2138  2002  4490\n",
            "  2061 27243  2035  1996  2051   102     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0], shape=(100,), dtype=int32)\n",
            "tf.Tensor(\n",
            "[  101  2066  1037 11478  2990  1051  1005 12856  1010  2049  6835 18874\n",
            "  2003  5173  2013  1037  8840 18384 16940  1010  2383  2018  2035  2049\n",
            "  8995 11305 20804  2041  1998 15105   102     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0], shape=(100,), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vakqzatl3le0"
      },
      "source": [
        "# **Définition et utilisation du modèle distilBERT avec Keras**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uE_TEftF1OCf"
      },
      "source": [
        "Les données d'entrées étant maintenant correctement préparées, commençons par définir le modèle distilBERT pour ensuite l'appliquer aux données afin de réaliser l'opération de prolongation lexicale.  \n",
        "\n",
        "<img src=\"https://github.com/AlexandreBourrieau/ML-F1/blob/master/Carnets%20Jupyter/Images/DistilBERT_process.png?raw=true\"/>  \n",
        "La fonction `model()` permet d'exécuter le modèle sur les séquences d'entrées."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38a0wigo18Zg",
        "outputId": "b7daff1e-ffca-49ac-cac9-7deaef4278d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "# Configuration du modèle distilBERT\n",
        "config = DistilBertConfig(num_labels=1)           # 1 label\n",
        "config.output_hidden_states = False               # Ne récupère pas la totalité des couches mais uniquement la dernière\n",
        "\n",
        "# Instanciation du modèle distilBERT\n",
        "transformer_model = TFDistilBertModel.from_pretrained('distilbert-base-uncased', config = config)\n",
        "\n",
        "# Défintion du format des entrées du modèle\n",
        "input_ids_in = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), name='input_token', dtype='int32')\n",
        "input_masks_in = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), name='masked_token', dtype='int32') \n",
        "\n",
        "# Création de la sortie du modèle\n",
        "sortie_distilBERT = transformer_model(input_ids_in, attention_mask=input_masks_in)"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_layer_norm', 'activation_13', 'vocab_projector', 'vocab_transform']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9vwylPK8B2d"
      },
      "source": [
        "Pour chaque commentaire en entrée, la sortie du modèle distilBERT est un vecteur de dimension MAX_SEQUENCE_LENGTHx768 :\n",
        "* Il y a au maximum MAX_SEQUENCE_LENGTH mots dans chaque commentaire\n",
        "* Il y a un vecteur en sortie du modèle par mot dans chaque commentaire\n",
        "* Le vecteur qui code chaque mot est de dimension 768"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84jTmbNS9och"
      },
      "source": [
        "Vérifions cela en regardant le format de la sortie du modèle :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRyx0t5z5_zY",
        "outputId": "56e08710-c974-473d-cddf-97efbf6df8c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "sortie_distilBERT"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor 'tf_distil_bert_model_20/distilbert/transformer/layer_._5/output_layer_norm/batchnorm/add_1:0' shape=(None, 100, 768) dtype=float32>,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puRHTFsg9x0d"
      },
      "source": [
        "Parmi les MAX_SEQUENCE_LENGTH vecteurs en sortie, il ne nous faut que le premier (celui qui correspond au mot clé [CLS]). On doit donc récupérer, pour chaque commentaire, le premier vecteur de dimension 768 parmi les MAX_SEQUENCE_LENGTH en sortie :  \n",
        "  \n",
        "  \n",
        "<img src=\"https://github.com/AlexandreBourrieau/ML-F1/blob/master/Carnets%20Jupyter/Images/Slice_SortieBERT.png?raw=true\"/>  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7R2nn5n7sww"
      },
      "source": [
        "sortie_CLS = sortie_distilBERT[0][:,0,:]"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMtLGYG44Kxh",
        "outputId": "a35f3b12-ff5f-4f2f-af87-e401eaddaa36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "model = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs = sortie_CLS)\n",
        "model.summary()"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_19\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_token (InputLayer)        [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "masked_token (InputLayer)       [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_distil_bert_model_20 (TFDist ((None, 100, 768),)  66362880    input_token[0][0]                \n",
            "                                                                 masked_token[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_24 (T [(None, 768)]        0           tf_distil_bert_model_20[0][0]    \n",
            "==================================================================================================\n",
            "Total params: 66,362,880\n",
            "Trainable params: 66,362,880\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEMRlzgpHlV2"
      },
      "source": [
        "Exécutons maintenant distilBERT sur les 10 premiers commentaires afin de regarder le format des sorties obtenues :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYJ7h-RC43o4",
        "outputId": "ea559e28-bed8-45d6-d6c4-2399026df7c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "sortie_prolongation_lexicale = model.predict([output_tokenizer_entrainement['input_ids'][1:10],output_tokenizer_entrainement['attention_mask'][1:10]],verbose=1)\n",
        "print(\"Commentaire :\",commentaires[0])\n",
        "print(\"input_ids\", output_tokenizer_entrainement['input_ids'][0])\n",
        "print(\"Sortie BERT\", sortie_prolongation_lexicale[0])"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 4ms/step\n",
            "Commentaire : a stirring , funny and finally transporting re imagining of beauty and the beast and 1930s horror films\n",
            "input_ids tf.Tensor(\n",
            "[  101  2062  1997  1996  2168  2214 13044  5365  2038  2042  2667  2000\n",
            "  3413  2125  2004 11701  9458  4024  2005  2070  2051  2085   102     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0], shape=(100,), dtype=int32)\n",
            "Sortie BERT [-2.66080976e-01 -4.85941917e-02  7.22826123e-02 -2.84811229e-01\n",
            " -1.45506337e-01 -1.00999802e-01  2.60227740e-01  4.30319756e-01\n",
            " -2.96179414e-01 -7.76587240e-03  9.90856625e-03 -1.17969126e-01\n",
            "  8.30014981e-03  2.70279408e-01 -1.40367121e-01  8.56669322e-02\n",
            " -1.03240475e-01  3.75994742e-01  1.79186329e-01 -2.12135017e-01\n",
            "  1.04085468e-01 -2.79367119e-01 -1.91953897e-01 -2.17457995e-01\n",
            " -1.43001378e-02 -3.40541452e-02  2.83225086e-02 -2.49343514e-02\n",
            "  2.44267598e-01 -1.43557250e-01 -3.73612121e-02  9.85163227e-02\n",
            "  6.92061521e-03 -6.01170808e-02  4.51826416e-02  1.06599465e-01\n",
            "  7.70470202e-02 -1.58807233e-01 -7.63385836e-03  6.24679513e-02\n",
            " -1.08122215e-01 -3.52480412e-02  3.35929513e-01 -2.73086131e-02\n",
            "  3.20224091e-03 -1.74682662e-01 -2.19190741e+00 -3.86436135e-02\n",
            " -1.97187617e-01 -3.45657289e-01  1.38873905e-01 -4.47584689e-03\n",
            "  2.00182885e-01  3.08498107e-02  2.73172557e-01  3.38543475e-01\n",
            " -1.35453105e-01  4.12504405e-01  1.06570810e-01 -1.97214112e-02\n",
            " -4.54099476e-03 -2.06217915e-02 -3.27952094e-02 -7.01349229e-04\n",
            " -1.31431520e-01  1.24997377e-01 -9.70280766e-02  2.33466789e-01\n",
            " -1.75274938e-01  4.53216851e-01 -1.57614484e-01 -2.01409787e-01\n",
            "  1.63659245e-01  8.69517177e-02  6.02823794e-02 -9.77548137e-02\n",
            "  3.41429114e-02  3.74584906e-02 -7.94879198e-02  9.21175629e-02\n",
            " -6.05647191e-02  1.39784753e-01  4.02011573e-01  2.75777876e-01\n",
            "  3.40925693e-03  2.62185007e-01 -2.57567704e-01 -1.25676557e-01\n",
            "  1.28736317e-01  3.73552591e-01 -2.97419429e-01 -7.96544179e-03\n",
            " -2.25009769e-02  1.46028161e-01  2.00626493e-01 -2.01698542e-01\n",
            " -4.39181849e-02 -2.07493268e-03  2.86091089e-01  1.90074101e-01\n",
            "  2.50496864e-02 -7.94637725e-02  1.34384185e-01 -1.96795031e-01\n",
            " -1.98950432e-02 -3.21254414e-03 -2.30551660e-02 -1.93652123e-01\n",
            "  1.24751776e-03 -2.38565588e+00  2.15531528e-01  2.72807330e-01\n",
            "  4.20747586e-02 -2.12256104e-01 -6.70752674e-02  3.68285209e-01\n",
            "  1.20085374e-01 -6.58920966e-03 -2.03918770e-01 -1.26867555e-02\n",
            " -1.11367404e-01  3.61413479e-01  1.10972568e-01 -1.91417664e-01\n",
            "  1.76879019e-01  2.21341178e-01 -8.86752754e-02  4.59257811e-02\n",
            "  1.21490598e-01  1.67598069e-01  2.40078598e-01  3.42349827e-01\n",
            " -6.10983334e-02 -2.34012455e-01 -1.04837611e-01  2.06686258e-01\n",
            "  2.05323100e-01 -1.95792411e-03 -3.60149965e-02  7.48931244e-02\n",
            " -2.66079336e-01  1.38075739e-01 -3.07319307e+00  2.99984515e-01\n",
            "  3.93409193e-01  4.74476852e-02 -1.05743535e-01 -1.65666975e-02\n",
            " -2.71908231e-02  1.96823001e-01  1.11611918e-01  2.42083386e-01\n",
            " -8.30935985e-02  4.94398922e-02 -1.59787089e-01  1.28489405e-01\n",
            " -1.95108235e-01 -5.91171868e-02  1.53489009e-01  1.52930200e-01\n",
            " -1.79973990e-02 -9.71024930e-02 -7.62917101e-02 -2.03568429e-01\n",
            " -1.17976166e-01  4.79334779e-02  3.25144649e-01  8.29094499e-02\n",
            "  3.39751281e-02  2.27281794e-01 -6.64264858e-02 -1.42946597e-02\n",
            "  5.46634436e-01 -1.42264098e-01  2.46845230e-01  2.53321975e-02\n",
            "  1.18593566e-01  2.50134796e-01 -1.45274168e-02  3.47643830e-02\n",
            " -7.89031386e-03  3.02761912e-01  1.61561638e-01  1.55645192e-01\n",
            " -8.54361504e-02 -8.93635005e-02  2.39106670e-01 -2.84734547e-01\n",
            "  1.79599047e-01  1.82338506e-01 -2.45134346e-02 -6.13917187e-02\n",
            "  1.21752873e-01  6.40107244e-02  8.14917684e-02  1.01788506e-01\n",
            "  1.06183395e-01 -5.30452132e-01  2.39751130e-01 -2.75089741e-02\n",
            " -1.02026999e-01  1.15953665e-02 -2.20092461e-02 -1.37679689e-02\n",
            " -3.99059504e-01  3.65414381e+00  1.26885101e-02 -1.94125809e-02\n",
            "  1.35690734e-01  2.25459769e-01 -3.36380601e-02  3.90965864e-02\n",
            "  1.97488442e-02 -2.06770733e-01  1.69715196e-01  3.07030845e-02\n",
            "  2.98400998e-01 -4.80112061e-03  3.26978527e-02 -4.23272997e-02\n",
            " -1.55077204e-02  1.74962163e-01 -9.18340385e-02  2.06661463e-01\n",
            "  3.66956741e-03  1.77116930e-01  1.06935441e-01  6.08498007e-02\n",
            " -4.94443439e-03 -9.87839818e-01  4.24080379e-02 -1.77761108e-01\n",
            " -5.43375313e-03  2.65144408e-01 -2.05274671e-01 -2.36155465e-02\n",
            "  6.65861666e-02 -2.14646626e-02  1.72267452e-01  4.14784700e-02\n",
            " -1.21020250e-01  6.46462739e-02  3.55275393e-01  1.23849086e-01\n",
            " -1.27964169e-01  1.87150553e-01  2.10757777e-01 -2.90655103e-02\n",
            " -8.39206278e-02 -5.05771749e-02  2.96283066e-01 -4.16013300e-02\n",
            "  1.46093853e-02 -2.16544032e-01  9.21422765e-02 -7.40028024e-02\n",
            "  3.26251648e-02  1.24057829e-02 -2.53092051e-01 -9.04283673e-02\n",
            " -1.80174232e-01 -6.87425658e-02  1.95102736e-01  5.27801849e-02\n",
            " -3.03338587e-01  1.00591794e-01 -6.70366436e-02 -1.83509842e-01\n",
            "  1.68177798e-01 -8.32294673e-02 -1.82859838e-01  2.09909618e-01\n",
            " -3.03729385e-01 -3.85502434e+00  5.46969660e-02 -1.78224385e-01\n",
            "  2.79422164e-01  1.50137216e-01 -1.31327450e-01 -7.68305808e-02\n",
            "  1.37170315e-01  3.62978429e-01 -3.14174920e-01  2.45678008e-01\n",
            "  7.06034899e-02 -6.69364333e-02  1.17949493e-01 -2.59348094e-01\n",
            "  1.56084523e-01  7.44818673e-02 -5.48732430e-02  2.84347683e-03\n",
            " -8.23418498e-02 -1.55758224e-02  2.27469742e-01 -2.48897880e-01\n",
            "  3.47567722e-04  6.51640594e-02 -1.90334201e-01 -2.36577362e-01\n",
            " -5.41608222e-02  1.00160927e-01 -1.58065502e-02 -7.46625066e-02\n",
            " -1.41379043e-01  2.03657106e-01 -4.96403612e-02 -3.93141329e-01\n",
            " -2.66107631e+00 -5.71136624e-02 -1.35506704e-01 -9.66212600e-02\n",
            "  2.36437529e-01 -1.67193621e-01  1.77723423e-01 -2.44443923e-01\n",
            " -1.78116396e-01  2.38254771e-01  8.32480267e-02 -6.14151955e-02\n",
            "  1.13340840e-01  9.97360498e-02  3.00343305e-01  1.07412040e-02\n",
            "  1.92189112e-01 -1.31845653e-01  6.60190433e-02  3.41437906e-02\n",
            "  1.24623753e-01  1.24366671e-01  2.54372377e-02  2.88763065e-02\n",
            " -2.52695233e-02  1.63213819e-01 -1.82593808e-01 -1.10587291e-01\n",
            " -3.32251281e-01 -1.10379197e-01 -1.26948163e-01 -1.49780691e-01\n",
            "  4.41177413e-02  1.26394853e-02 -2.52636015e-01 -1.22122131e-01\n",
            "  5.37338033e-02  1.39300942e-01  4.06074882e-01  2.09548101e-01\n",
            " -1.76801711e-01  4.36073095e-01  8.46588165e-02  1.47315606e-01\n",
            "  4.29580003e-01  5.87006211e-02  7.71611780e-02 -1.99895024e-01\n",
            " -1.09589890e-01  1.44656762e-01 -4.26797122e-02  8.38672668e-02\n",
            "  9.60449576e-01 -9.27485228e-02  2.21923918e-01 -1.21896014e-01\n",
            "  3.71511698e-01  9.76347029e-02  4.87229079e-02 -1.15223214e-01\n",
            "  4.23492163e-01  3.89295295e-02  1.10909171e-01 -1.17975883e-02\n",
            "  1.05323642e-03 -2.52507687e-01  2.62192726e-01 -2.06458822e-01\n",
            "  1.54160429e-02  3.44572216e-03 -8.59834403e-02  4.24291193e-02\n",
            "  1.17244974e-01 -8.40241730e-01 -2.14604929e-01  1.98686689e-01\n",
            " -2.02584103e-01  6.94713444e-02  2.26059593e-02  1.58471968e-02\n",
            " -1.86579019e-01 -2.15288967e-01  5.20989075e-02  2.75188684e-01\n",
            " -2.57585377e-01 -2.04594374e-01 -2.83267736e-01 -1.06382638e-01\n",
            " -2.98760027e-01  5.14316633e-02 -1.26618505e-01  1.51089519e-01\n",
            "  1.13913946e-01 -1.38416067e-02  3.92607823e-02  1.78171635e-01\n",
            "  2.16881037e-01 -6.83690071e-01  1.28441066e-01 -2.16441661e-01\n",
            " -2.95760036e-02 -2.10894436e-01  3.55310142e-02  1.10059917e-01\n",
            " -5.76371476e-02 -1.27320856e-01 -1.67884439e-01  3.24283421e-01\n",
            " -6.53283894e-02  1.41824186e-01 -3.83959487e-02  1.50165483e-01\n",
            "  7.70389140e-02  7.41017312e-02  8.48312855e-01 -3.87185290e-02\n",
            "  6.38358444e-02  3.08548987e-01 -4.31107953e-02  2.33508661e-01\n",
            "  2.03861356e-01  1.29928857e-01 -1.16401277e-02 -1.48006305e-02\n",
            "  4.30677682e-02 -7.05404505e-02  4.85840328e-02 -2.74950504e-01\n",
            " -3.65721136e-01  1.14628419e-01  8.88618529e-02 -8.72452706e-02\n",
            "  3.78633849e-03 -6.81834698e-01 -7.53531829e-02 -2.82635182e-01\n",
            " -3.02575678e-01  1.39269792e-03  2.69360185e-01  1.10410199e-01\n",
            "  2.20937878e-01  8.99035558e-02  3.48492227e-02  4.64258045e-01\n",
            " -6.57942370e-02  3.72041821e-01  6.77511096e-02  1.95862576e-02\n",
            " -6.34756535e-02  2.55301416e-01 -1.40303060e-01 -2.11832508e-01\n",
            "  9.46566910e-02 -2.24086910e-01  1.08235128e-01 -6.99643344e-02\n",
            "  9.01588425e-03  5.49848676e-02 -5.87652251e-03 -2.57242732e-02\n",
            " -7.06535727e-02  6.46699369e-02 -1.19868433e+00  4.21070695e-01\n",
            "  6.25221729e-02  6.62271008e-02  1.41630277e-01 -1.99941933e-01\n",
            " -1.80638626e-01  3.58233511e-01  2.00108230e-01  1.94891945e-01\n",
            " -3.55434939e-02  9.81056243e-02 -8.71295854e-02  1.08727515e-01\n",
            " -5.55390567e-02 -3.86405513e-02  1.33398414e-01  3.85622680e-03\n",
            "  4.01906520e-02  4.18721437e-02 -1.69721022e-02  2.69066423e-01\n",
            "  8.91907886e-02 -1.80025429e-01 -6.66025206e-02 -2.02908237e-02\n",
            "  1.41660627e-02  3.71668458e-01 -2.23867595e-03  2.30321646e-01\n",
            "  1.20909937e-01 -2.74118841e-01 -5.18620193e-01  5.10306470e-03\n",
            "  1.67512715e-01  1.68186694e-01 -1.17240548e-02 -1.62522689e-01\n",
            "  1.39779270e-01 -1.38490349e-01 -3.95043522e-01  2.40595981e-01\n",
            "  1.15742758e-01 -2.63777047e-01  4.78197336e-01  1.63118541e-01\n",
            " -2.64781177e-01  1.74070746e-01 -2.36311443e-02 -2.73866177e-01\n",
            "  5.36802486e-02 -1.57095730e-01 -3.92600372e-02 -1.56630099e-01\n",
            " -1.18925190e-02  8.74790102e-02 -8.28203857e-02 -1.25401229e-01\n",
            " -2.35583439e-01 -1.05305433e-01  3.71193111e-01 -2.60231286e-01\n",
            " -2.11376905e-01  1.80826336e-01 -1.33835346e-01 -4.00579184e-01\n",
            " -7.95202479e-02  2.33447254e-02  9.21287313e-02  1.22502312e-01\n",
            "  2.62521535e-01  5.43967113e-02 -7.59469345e-03  2.23674655e-01\n",
            " -2.14919776e-01  2.62976497e-01  1.42767280e-01  1.53526634e-01\n",
            "  2.66412824e-01 -1.01870909e-01  3.16362791e-02 -1.34158835e-01\n",
            " -2.50684649e-01  2.24623978e-01 -1.15581334e-01 -5.11565208e-02\n",
            " -1.46593779e-01  1.50997937e-01  6.76991865e-02  9.41400230e-03\n",
            " -1.64872184e-01 -2.24747002e-01  7.65409395e-02 -5.16717806e-02\n",
            "  8.55913907e-02  5.20246811e-02  4.06668521e-02 -3.06238718e-02\n",
            " -9.75773409e-02  7.97697902e-03  1.45502657e-01  2.99000740e-01\n",
            "  2.17068911e-01  5.99293895e-02  1.31445020e-01  1.55399203e-01\n",
            "  3.33928883e-01  8.27098191e-02 -1.54707074e-01  1.31922975e-01\n",
            "  1.09826177e-02 -8.13464150e-02  4.34823483e-02  8.09586644e-02\n",
            " -3.31324637e-02 -3.68722528e-01 -6.47051930e-02 -1.74476981e-01\n",
            "  2.03339100e+00  3.60276341e-01 -2.91291103e-02  1.38016194e-02\n",
            "  9.80367735e-02  2.97396909e-02 -3.30662057e-02  1.19490743e-01\n",
            " -1.59003809e-01  1.84119910e-01  1.58428058e-01  1.94984406e-01\n",
            "  6.03230298e-03  2.34371245e-01  2.19013661e-01  1.94085330e-01\n",
            "  2.79478729e-03 -2.87744343e-01 -4.20516044e-01 -3.46095711e-02\n",
            " -4.10212815e-01  6.71563074e-02  2.23243311e-01  1.44088790e-02\n",
            " -6.65123016e-03  9.52220112e-02  7.10367858e-02 -2.53603697e-01\n",
            "  5.99042140e-03  1.24147773e-01 -5.73140047e-02 -3.46637191e-03\n",
            " -7.02337176e-02  3.48114282e-01 -7.82261863e-02  3.44113335e-02\n",
            "  6.35020807e-02 -2.05792412e-01 -1.35929003e-01  2.20144361e-01\n",
            "  6.22860193e-02 -3.81628305e-01  3.20059806e-01  7.75931999e-02\n",
            "  1.22526586e-01  3.78567994e-01 -1.24321982e-01 -2.42445931e-01\n",
            "  4.41113323e-01  2.10495755e-01 -2.27765426e-01 -2.59158462e-01\n",
            " -1.08846903e-01  1.82415433e-02  7.31689017e-03 -1.34035528e-01\n",
            "  3.79230082e-02 -6.99256361e-02 -1.79548487e-01  2.84072638e-01\n",
            " -2.32052766e-02  8.42296705e-02  1.23030975e-01 -9.65924859e-02\n",
            "  2.75553674e-01  1.09672785e-01 -2.24640548e-01  5.02293259e-02\n",
            "  8.03167149e-02 -1.50350913e-01 -5.15923090e-03  2.90654719e-01\n",
            "  1.57616436e-01  8.45349580e-02  2.64841437e-01  2.53737476e-02\n",
            "  2.67574131e-01 -1.08317122e-01 -1.73950881e-01 -2.84961057e+00\n",
            "  1.27588868e-01 -1.01166442e-01 -1.31171107e-01  3.05239260e-02\n",
            "  3.13890249e-01  1.51305467e-01 -2.23409489e-01  3.45265530e-02\n",
            " -2.35636160e-01  3.16617489e-01  3.83485198e-01  2.77799368e-01\n",
            "  1.18197262e-01  1.94595858e-01  1.38571471e-01  1.24067262e-01\n",
            " -1.69436336e-01  2.28919759e-02  4.35307808e-02 -4.08545285e-02\n",
            " -8.86695087e-03  1.93511322e-02 -2.41183460e-01 -1.99930519e-01\n",
            "  2.14929536e-01 -2.16268539e-01 -1.29992709e-01 -1.10584155e-01\n",
            " -4.50500175e-02 -1.20351717e-01  8.21086690e-02 -4.93857674e-02\n",
            "  1.91574283e-02 -1.25950456e-01 -1.58789456e-01 -1.46723494e-01\n",
            " -8.42568427e-02  7.13528171e-02  4.93779406e-02  3.36692482e-03\n",
            "  3.36764038e-01  2.53812186e-02  5.42403832e-02 -4.72753569e-02\n",
            "  4.44865897e-02  3.36532086e-01 -2.52029449e-01  2.74116009e-01\n",
            " -4.62828390e-02  1.42875835e-02  3.25892791e-02  2.24468365e-01\n",
            "  1.06576748e-01  1.90853477e-01  1.09298319e-01 -1.02868289e-01\n",
            " -1.87509693e-04 -1.02304943e-01 -2.73049563e-01 -9.39884037e-03\n",
            "  3.37737054e-03 -4.50550318e-02  1.15328647e-01  1.79617882e-01\n",
            " -5.84151261e-02 -8.39228556e-02 -1.79600030e-01 -3.42575870e-02\n",
            " -5.48168495e-02 -2.12576047e-01 -1.24982677e-01  2.95541763e-01\n",
            "  5.65323830e-02  6.57526478e-02 -1.06027745e-01  3.17225695e-01\n",
            "  1.68522522e-01  4.90477979e-02  7.56101906e-02  7.39011317e-02\n",
            "  1.50648663e-02 -1.31587405e-02 -6.45266026e-02  2.39284068e-01\n",
            " -7.74212885e+00 -2.93458849e-01  2.29821000e-02 -1.36994272e-02\n",
            " -1.41681075e-01 -2.12347537e-01  2.62804963e-02 -1.68972924e-01\n",
            " -8.53287727e-02 -2.84611378e-02  2.94134408e-01  2.19789110e-02\n",
            " -1.12207621e-01 -1.03825979e-01  1.89206302e-01  2.60748893e-01]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}