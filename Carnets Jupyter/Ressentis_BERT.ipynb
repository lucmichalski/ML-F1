{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPq996pij0FfgRqrCwcilsg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexandreBourrieau/ML-F1/blob/master/Carnets%20Jupyter/Ressentis_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJih03NWtfi0"
      },
      "source": [
        "# **Classification de ressentis avec distilBERT**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6Xo1x4rtto8"
      },
      "source": [
        "L'objectif est de créer un modèle qui prend en entrée des commentaires (en Anglais) et attribue à chacun un ressenti positif ou négatif.  \n",
        "Le modèle est composé de deux parties :  \n",
        "* DistilBERT va encoder le commentaire et en extraire des informations qui seront passées ensuite au réseau de neurones.  \n",
        "* Le modèle suivant est un réseau de neurones qui sera créé avec Keras.  \n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/AlexandreBourrieau/ML-F1/master/Carnets%20Jupyter/Images/StructureBERT.png\" />  \n",
        "  \n",
        "  Les données qui s'échangent entre les deux modèles sont des vecteurs de dimension 768. Ces vecteurs sont l'équivalent de l'application d'un algorithme de prolongation lexicale sur les mots qui composent le commentaire."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gePh7FotwPTH"
      },
      "source": [
        "# **Installation et importations des librairies**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5WrrT_pzIHu",
        "outputId": "4c2b53ef-bda9-4c75-8ae1-43d158a600a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "!pip install transformers --quiet"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.1MB 2.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.0MB 19.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 34.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 35.3MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5n4uHtutPlJ"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Dropout, Input, Dropout, Lambda\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from transformers import TFDistilBertModel\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import random"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWZEaivhxTd2"
      },
      "source": [
        "# **Importation des données**\n",
        "\n",
        "On utilise la librairie pandas pour lire les données depuis le fichier csv disponible sur le site de [standford](https://nlp.stanford.edu/sentiment/index.html) qui contient des commentaires sur des films, chacun d'eux avec une note positive (1) ou négative (0)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOrjCZGyyH4P"
      },
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/AlexandreBourrieau/ML-F1/master/Carnets%20Jupyter/Donn%C3%A9es/train.csv', delimiter='\\t', header=None)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiO-lNQ6zYpU"
      },
      "source": [
        "Affiche quelques informations :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQT1SwyBzPvP",
        "outputId": "b220dd09-0cfe-4fff-d394-4f7b983c35f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        }
      },
      "source": [
        "print(df[0:10])\n",
        "print(\"Total des données : \", str(len(df)))\n",
        "print(\"Nombre d'avis positifs et négatifs : \",df[1].value_counts())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                   0  1\n",
            "0  a stirring , funny and finally transporting re...  1\n",
            "1  apparently reassembled from the cutting room f...  0\n",
            "2  they presume their audience wo n't sit still f...  0\n",
            "3  this is a visually stunning rumination on love...  1\n",
            "4  jonathan parker 's bartleby should have been t...  1\n",
            "5  campanella gets the tone just right funny in t...  1\n",
            "6  a fan film that for the uninitiated plays bett...  0\n",
            "7  b art and berling are both superb , while hupp...  1\n",
            "8  a little less extreme than in the past , with ...  0\n",
            "9                       the film is strictly routine  0\n",
            "Total des données :  6920\n",
            "Nombre d'avis positifs et négatifs :  1    3610\n",
            "0    3310\n",
            "Name: 1, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDDdUWj83wuz"
      },
      "source": [
        "# **Préparation des données**\n",
        "\n",
        "# Tokénisation  \n",
        "La première étape est de tokéniser les commentaires : les mots sont décomposés en index numériques au format BERT.  \n",
        "\n",
        "<img src=\"https://github.com/AlexandreBourrieau/ML-F1/blob/master/Carnets%20Jupyter/Images/TokenizeBERT.png?raw=true\"/>\n",
        "\n",
        "Après tokénisation, on obtient une liste de séquences et chaque séquence représente une liste d'index. On souhaite que BERT analyse toutes les séquences en une seule fois (ce qui est plus rapide). Il faut donc que toutes les séquences aient la même taille. On va donc ajouter du bourrage pour égaliser la longueur des séquences. Cela est indiqué avec le paramètre `padding='True'`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgilnUOI32Bk"
      },
      "source": [
        "MAX_SEQUENCE_LENGTH = 500\n",
        "\n",
        "# Chargement des commentaires et des ressentis\n",
        "commentaires = df[0]                                   # Récupère tous les commentaires dans une dataframe pandas\n",
        "ressentis = df[1]                                      # Récupère tous les ressentis dans une liste python\n",
        "labels = np.asarray(ressentis)                         # Créé un tableau de type numpy avec les ressentis\n",
        "\n",
        "x_entrainement, x_test, y_entrainement, y_test = train_test_split(commentaires, labels, test_size=0.25)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHhqUPeiqzam"
      },
      "source": [
        "# Instanciation du tokeniseur\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Préparation des données d'entrainement\n",
        "output_tokenizer_entrainement = x_entrainement.apply((lambda x: tokenizer.encode(x,max_length=MAX_SEQUENCE_LENGTH, padding='longest', truncation=True, return_tensors='tf',add_special_tokens=True)))\n",
        "\n",
        "# Préparation des données de tests\n",
        "output_tokenizer_tests = x_test.apply((lambda x: tokenizer.encode(x,max_length=MAX_SEQUENCE_LENGTH, padding='longest', truncation=True, return_tensors='tf',add_special_tokens=True)))"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzboG3fX5LkP"
      },
      "source": [
        "Regardons un peu comment sont formatées les données en sortie du tokéniseur :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-E6oJcG5Qlj",
        "outputId": "86235bf0-5243-460c-b0fa-2f5a5b24c6a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "print(\"Commentaire original :\", x_entrainement[1])\n",
        "print(\"Résultat de la tokénisation: \", output_tokenizer_entrainement[1])"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Commentaire original : apparently reassembled from the cutting room floor of any given daytime soap\n",
            "Résultat de la tokénisation:  tf.Tensor(\n",
            "[[  101  4593  2128 27241 23931  2013  1996  6276  2282  2723  1997  2151\n",
            "   2445 12217  7815   102]], shape=(1, 16), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAmGZYP86Hx7"
      },
      "source": [
        "Regardons les 5 premiers résutats de la tokénisation : On peut identifier les mot-clés **[CLS]** (valeur : 101) et **[SEP]** (valeur : 102)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWVaSBqo59qW",
        "outputId": "d5c044f3-f4f4-459c-980f-e112672f20d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        }
      },
      "source": [
        "for i in range (0,5):\n",
        "  print(output_tokenizer_entrainement[i])"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[  101  1037 18385  1010  6057  1998  2633 18276  2128 16603  1997  5053\n",
            "   1998  1996  6841  1998  5687  5469  3152   102]], shape=(1, 20), dtype=int32)\n",
            "tf.Tensor(\n",
            "[[  101  4593  2128 27241 23931  2013  1996  6276  2282  2723  1997  2151\n",
            "   2445 12217  7815   102]], shape=(1, 16), dtype=int32)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2890\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2891\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2892\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 2",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-3c7594c6256d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tokenizer_entrainement\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    880\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m         if (\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    990\u001b[0m         \u001b[0;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 991\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    992\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_values_for_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2891\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2892\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2893\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2895\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 2"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfZCNYWBNjnz"
      },
      "source": [
        "# Bourrage\n",
        "Après tokénisation, `output_tokenizer` est une liste de séquences, chaque séquence représente une liste d'index. On souhaite que BERT analyse toutes les séquences en une seule fois (ce qui est plus rapide). Il faut donc que toutes les séquences aient la même taille. On va donc ajouter du bourrage pour égaliser la longueur des séquences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahRN79-SOtFY"
      },
      "source": [
        "max_len = 0\n",
        "for i in tokenized.values:\n",
        "    if len(i) > MAX_SEQUENCE_LENGTH:\n",
        "        max_len = len(i)\n",
        "\n",
        "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vakqzatl3le0"
      },
      "source": [
        "# **Définition du modèle**"
      ]
    }
  ]
}